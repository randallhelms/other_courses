**MapReduce and HDFS/S3**

*MapReduce and HDFS and Amazon EMR with S3*

	MapReduce inputs come from input data evenly distributed across servers - storage structure needs scalable and rapid access infrastructure, i.e. HDFS/S3 
	Each mapping job loads files on server and process files there
	Data processed in chunks, not on central server
	Storage structure needs to support this
	Number of maps usually driven by number of DFS blocks
	HDFS addresses fault tolerance for failure recovery, and tunable replication of data
	
	HDFS is scalable and highly fault tolerant
	Data stored in replicated blocks; (default 3 x)
	Store in S3, process with Elastic MapReduce (EMR)
	Allows you to shut down and stop paying when not using data
	
*HDFS Data Storage*

	Client checks NameNode to see where to write file to HDFS
	NameNode checks and finds DataNode where it can write file
	Client takes this info and writes block directly to DataNode, then this block is divided if it is larger than default block size
	DataNode replicates the data

*HDFS Data Storage: Blocks*

	HDFS file consists of number of blocks
	Block size is 64 mb by default
	Replication usually set to 3 nodes
	If failure occurs, data can be replicated from another nodes
	
*HDFS Data Storage: InputSplit*

	Input to map phase might require data from many blocks, and/or data stretched across multiple blocks
	InputSplits might not fit block boundary
	Example: Transaction data in one file for each year, with each transaction on a separate row
	Transaction data in chunks of 64mb
	HDFS can't gauge when record spills over to another block
	InputSplit is a logical representation of data stored in file blocks
	MapReduce job uses this to figure out first and last whole records
	Number of InputSplits determines number of Mapper tasks
	
*Amazon S3 buckets*

	Cloud-based object stores for files using URL
	Buckets are similar to internet domain names and should be unique
	You can organize and name objects in any way you like