**Map Reduce Framework**

*Introduction*

Before MapReduce big data required:

	Managing hundreds/thousands of processors
	Managing parallelization and distribution
	I/O scheduling
	Status and monitoring
	
MapReduce:

	Easy way to process vast amounts of data in-parallel on large clusters of commodity hardware
	
Path: Map phase -> User-defined task -> Reduce phase

*MapReduce framework benefits*

	Highly scalable
	Reduces complexity of running distributed data processing functions
	Can be deployed on large clusters of cheap hardware
	Works in a fault-tolerant manner; ensures that jobs fail independently and restart automatically
	Jobs scheduled in optimized way
	Supports multiple programming languages
	Supports technologies (Hive and Pig) to do background processing
	Allows for efficient processing structured/unstructured data
	
*MapReduce jobs*

Top level of work is a job, with both map and reduce phase

	Split input dataset into independent chunks
	Map tasks process data in parallel manner
	Framework sorts output of maps
	Input to reduce tasks
	Reduce phase takes map results as input for parallel reduce tasks
	Consolidate data into final results
	
MapReduce takes input as key-value pairs, and these don't need to be of the same type at any step of the process

*MapReduce Phases*

Word count example: 
	
	Each line from input data file taken as input to individual mapper
	Logic used to tokenize line into word array to create key value pairs along wit count <word, count>
	MapReduce sorts, shuffles and groups all same keys together and produces intermediate key-value pairs
	Fed as input to reducer where counting is done and final output with key-value pairs
	Output written to HDFS
	
Phases:

	Map phase: creates list of data elements, passes it to the Mapper; i.e. "Apple Orange Mango" leads to "Apple,1","Orange,1","Mango,1"
	Shuffle/Sort phase: Process by which system sorts/transfers map outputs to the Reducers ar inputs
	Reduce phase: Aggregates values together, to create one output element for each list
	
*Combiners and Partitioners*

Combiners

	Run after completion of map task
	Helpful in decreasing size of transient data
	Mini-reducers that run in memory after the map phase
	Have same interface as reduces

Partitioners:

	Application code that defines how keys are assigned to Reducers
	Controls partitioning of keys of intermediate map outputs
	Uses simple hash of the key
	Can make Custom Partitioners

	
