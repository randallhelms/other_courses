**Data Analysis Using Pig Programming**

Pig is:

	* a platform for analyzing large datasets
	* Open source programming environment that simplifies complex data analysis tasks
	* Simple language (Pig Latin) for queries and data manipulation. Compiled into MapReduce jobs that run on Hadoop
	* Combines SQL data manipulation constructs with procedural programming of MapReduce
	
When should it be used?

	* Pipelining data - ETL jobs on Hadoop and batch processing, Pig can pipeline data to external application
	* Supports pipelining data from Pig to SQL
	* Iterative data processing and ad-hoc queries
	* Rapid development and testing. 1/20 number of lines of code compared to Java
	* Applications needing debugging on regular basis
	* Easy to implement, simple to used
	
Why pig?

	* Data Agnostic: operates on data and metadata
	* Flexible: language for parallel data processing
	* Easily controlled: Easily controlled and modifiable, easy integration, with optimizer
	* Fast: Processes data quickly, can store data at intermediate steps
	
Pig Architecture Overview

	Pig Latin: Dataflow language
	Pig Latin compiler: Converts Pig Latin into executable code in form of MapReduce jobs or it can spawn process where virtual Hadoop instance is created to run Pig code on single node
	
Pig Architecture Data Flow

	Primarily dataflow mechanism
	Supports concept of non-linear data flows
	Optimizer can recognize when same input is referenced multiple times
	
	Pig High Level Data Flow:
		Load
		Transform
		Dump / Store
